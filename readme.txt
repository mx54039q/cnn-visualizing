#不需要减均值：在源码中，读图像预处理有一步是减均值，使得输入的范围大概在[-105,150]；而在反卷积的data反向process之前，源码又对data进行了标准化，使得data的范围在[0,255]，这前后两步存在矛盾。在cnn中，减图像均值并不是这么重要，索性将预处理中的见均值操作去掉。

#源码中在将正向网络的pool5赋给反向网络的输入时，将小于150的置为0,这一步不能理解，既不是ReLU,所以索性去掉。

#源码中反向网络都没有加ReLU，与论文不符，所以按照论文加上了ReLU层

#源码中没有求特定层的最strong特征，只是将该层所有特征加入到反卷积中，所以用sum函数求特征层每一个特征的和，并求出最强特征的下标，并只对该特征进行反卷积

xs24
Sx89ha@34


#test